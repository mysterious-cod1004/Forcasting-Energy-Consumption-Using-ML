{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Energy Consumption Forecasting</h1>\n",
    "\n",
    "Real-time daily data for Energy Met and Daily Maximum Load at the national level taken from Grid-India (Grid Controller of India Limited) National Despatch Centre.\n",
    "\n",
    "Data source: https://report.grid-india.in/psp_report.php\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Extraction</h2>\n",
    "\n",
    "Using BeautifulSoup and requests libraries, downloading and saving daily data into folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "\n",
    "# Base URL of the website\n",
    "base_url = 'https://report.grid-india.in'\n",
    "\n",
    "# Function to generate the form data for a given date\n",
    "def generate_form_data(date):\n",
    "    return {\n",
    "        'selected_date': date.strftime('%Y-%m-%d')\n",
    "    }\n",
    "\n",
    "# Function to download a file from a URL\n",
    "def download_file(url, save_dir, file_name):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        file_path = os.path.join(save_dir, file_name)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded: {file_name}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {file_name}\")\n",
    "\n",
    "# Function to extract and download PDF reports from the form submission response\n",
    "def extract_and_download_pdfs(response_text, save_dir):\n",
    "    soup = BeautifulSoup(response_text, 'html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "    for link in links:\n",
    "        url = link['href']\n",
    "        if url.endswith('.pdf'):  # Check if the link is for a PDF file\n",
    "            if not url.startswith('http'):\n",
    "                url = base_url + url[1:]  # Ensure the URL is absolute\n",
    "            file_name = url.split('/')[-1]\n",
    "            download_file(url, save_dir, file_name)\n",
    "\n",
    "# Main function to download PDF reports over a date range\n",
    "def download_reports(start_date, end_date, save_dir):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    url = base_url + '/psp_report.php'  # Form submission URL\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        form_data = generate_form_data(current_date)\n",
    "        response = requests.post(url, data=form_data)\n",
    "        if response.status_code == 200:\n",
    "            extract_and_download_pdfs(response.text, save_dir)\n",
    "        else:\n",
    "            print(f\"Failed to fetch report for {current_date.strftime('%Y-%m-%d')}\")\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "# Example usage\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2020, 12, 31)\n",
    "save_dir = 'reports\\\\2020'\n",
    "\n",
    "download_reports(start_date, end_date, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through the folder containing the PDF files, extracting the required data from the files and writing it into csv files for later use.\n",
    "\n",
    "I'm doing this year-wise in order to incorporate minor changes in the format of the PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tabula import read_pdf\n",
    "import warnings\n",
    "\n",
    "count = 0\n",
    "\n",
    "# Function to process a single PDF file\n",
    "def process_pdf(pdf_path):\n",
    "    try:\n",
    "\n",
    "         # Suppress warnings from tabula\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Fallback font.*\")\n",
    "\n",
    "        # Extract date from the file path\n",
    "        parts = pdf_path.split(\"\\\\\")\n",
    "        date_part = parts[-1].split(\"_\")[0]\n",
    "        day, month, year = map(int, date_part.split(\".\"))\n",
    "        date = pd.to_datetime(f'{int(year) + 2000}-{month}-{day}')\n",
    "        \n",
    "        # Extract tables from page 2 \n",
    "        tables = read_pdf(pdf_path, pages=\"2\", multiple_tables=True)\n",
    "\n",
    "        # Check if any tables were found\n",
    "        if not tables:\n",
    "            print(f\"No tables found on page 2 of the PDF: {pdf_path}\")\n",
    "            return None, None\n",
    "\n",
    "        df = tables[2]\n",
    "        state_col = df.columns[1]\n",
    "        max_demand_column = df.columns[3]\n",
    "        energy_met_column = df.columns[5]\n",
    "        df.rename(columns={state_col: 'State', max_demand_column: 'Max.Demand', energy_met_column: 'Energy Met'}, inplace=True)\n",
    "        df = df[['State', 'Max.Demand', 'Energy Met']]\n",
    "\n",
    "        # Find the index to start from (Punjab)\n",
    "        index_to_keep_from = df[df['State'] == 'Punjab'].index[0]\n",
    "        index_to_stop=df[df['State'] == 'Tripura'].index[0]\n",
    "        df = df.loc[index_to_keep_from:index_to_stop]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return date, df\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {pdf_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Folder containing PDF files\n",
    "pdf_folder = \"reports\\\\2020\"\n",
    "\n",
    "# Initialize lists to store data for energy_met and maximum_demand dataframes\n",
    "energy_met_data = []\n",
    "maximum_demand_data = []\n",
    "\n",
    "# Iterate through PDF files in the folder\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "        date, df = process_pdf(pdf_path)\n",
    "        if date is not None and df is not None:\n",
    "            try:\n",
    "                states = df['State'].tolist()\n",
    "                energy_met_data.append([date] + df['Energy Met'].tolist())\n",
    "                maximum_demand_data.append([date] + df['Max.Demand'].tolist())\n",
    "                count += 1\n",
    "                print(f\"Dataframe appended: {date}   ...{count}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error appending data for date {date}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in energy_met_data:\n",
    "    if len(el)!=37:\n",
    "        print(energy_met_data.index(el)+1, len(el))\n",
    "\n",
    "\n",
    "print(len(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create energy_met and maximum_demand dataframes\n",
    "energy_met_df = pd.DataFrame(energy_met_data,columns=['Date'] + states)\n",
    "maximum_demand_df = pd.DataFrame(maximum_demand_data,columns=['Date'] + states)\n",
    "\n",
    "\n",
    "# Display the final dataframes\n",
    "print(\"Energy Met:\\n\", energy_met_df)\n",
    "print(\"\\nMaximum Demand:\\n\", maximum_demand_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_met_df.to_csv('reports\\\\new_csv\\\\energy_2020.csv', index=False,mode='a')\n",
    "maximum_demand_df.to_csv('reports\\\\new_csv\\\\max_2020.csv', index=False, mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troubleshoot and understand the format of erroneous PDF files, and then appending to the required csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabula import read_pdf\n",
    "\n",
    "# Specify the PDF file path\n",
    "pdf_path = \"reports\\\\2020\\\\21.03.20_NLDC_PSP.pdf\"\n",
    "\n",
    "# Extract tables from page 2 \n",
    "tables = read_pdf(pdf_path, pages=\"2\", multiple_tables=True)\n",
    "\n",
    "# Check if any tables were found\n",
    "if not tables:\n",
    "    print(\"No tables found on page of the PDF.\")\n",
    "else:\n",
    "    # Assuming there's only one table on page 2 (adjust if needed)\n",
    "    #print(tables)\n",
    "    df = tables[2]\n",
    "\n",
    "df.head()\n",
    "df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Function to process a single PDF file\n",
    "def process_pdf(pdf_path):\n",
    "    try:\n",
    "        # Suppress warnings from tabula\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Fallback font.*\")\n",
    "\n",
    "        # Extract tables from page 2 \n",
    "        tables = read_pdf(pdf_path, pages=\"2\", multiple_tables=True)\n",
    "\n",
    "        # Check if any tables were found\n",
    "        if not tables:\n",
    "            print(f\"No tables found on page 2 of the PDF: {pdf_path}\")\n",
    "            return None, None\n",
    "\n",
    "        df = pd.read_csv('test.csv')\n",
    "        #df = tables[2]\n",
    "        state_col = df.columns[1]\n",
    "        max_demand_column = df.columns[3]\n",
    "        energy_met_column = df.columns[5]\n",
    "        df.rename(columns={state_col: 'State', max_demand_column: 'Max.Demand',energy_met_column: 'Energy Met'}, inplace=True)\n",
    "        #df[['Max.Demand', 'Extra']] = df['Max.Demand'].str.split(' ', n=1, expand=True)\n",
    "        #df[['Energy Met', 'Extra2']] = df['Energy Met'].str.split(' ', n=1, expand=True)\n",
    "        df = df[['State', 'Max.Demand', 'Energy Met']]\n",
    "\n",
    "        # Find the index to start from (Punjab)\n",
    "        #df=df.drop(df.index[18]) drop a column of specified index\n",
    "        index_to_keep_from = df[df['State'] == 'Punjab'].index[0]\n",
    "        index_to_stop=df[df['State'] == 'Tripura'].index[0]\n",
    "        df = df.loc[index_to_keep_from:index_to_stop]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.to_csv('test.csv', index=False)\n",
    "        #df=df.drop(df.index[13])\n",
    "        #df=df.drop(df.index[26])\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.to_csv('test.csv', index=False,mode='a')\n",
    "\n",
    "        # Extract date from the file path\n",
    "        parts = pdf_path.split(\"\\\\\")\n",
    "        date_part = parts[-1].split(\"_\")[0]\n",
    "        day, month, year = map(int, date_part.split(\".\"))\n",
    "        date = pd.to_datetime(f'{int(year) + 2000}-{month}-{day}')\n",
    "\n",
    "        return date, df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {pdf_path}: {e}\")\n",
    "        return None, None\n",
    " \n",
    "# Path to the single PDF file\n",
    "pdf_path = \"reports\\\\2020\\\\21.03.20_NLDC_PSP.pdf\"\n",
    "\n",
    "# Initialize lists to store data for energy_met and maximum_demand dataframes\n",
    "energy_met_data = []\n",
    "maximum_demand_data = []\n",
    "\n",
    "# Process the single PDF file\n",
    "date, df = process_pdf(pdf_path)\n",
    "if date is not None and df is not None:\n",
    "    try:\n",
    "        states = df['State'].tolist()\n",
    "        energy_met_data.append([date] + df['Energy Met'].tolist())\n",
    "        maximum_demand_data.append([date] + df['Max.Demand'].tolist())\n",
    "        print(f\"Dataframe appended: {date}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error appending data for date {date}: {e}\")\n",
    "\n",
    "\n",
    "energy_met_df = pd.DataFrame(energy_met_data, columns=['Date'] + states)\n",
    "maximum_demand_df = pd.DataFrame(maximum_demand_data, columns=['Date'] + states)\n",
    "\n",
    "\n",
    "# Create energy_met and maximum_demand dataframes\n",
    "if energy_met_data and maximum_demand_data:\n",
    "    energy_met_df = pd.DataFrame(energy_met_data, columns=['Date'] + states)\n",
    "    maximum_demand_df = pd.DataFrame(maximum_demand_data, columns=['Date'] + states)\n",
    "\n",
    "    # Append dataframes to CSV file\n",
    "    energy_met_csv_path = \"reports\\\\new_csv\\\\energy_2020.csv\"\n",
    "    maximum_demand_csv_path = \"reports\\\\new_csv\\\\max_2020.csv\"\n",
    "    energy_met_df.to_csv(energy_met_csv_path, index=False, header=False,mode='a')\n",
    "    maximum_demand_df.to_csv(maximum_demand_csv_path, index=False,header=False,mode='a')\n",
    "    print(f\"Dataframes appended to CSV files: {energy_met_csv_path}, {maximum_demand_csv_path}\")\n",
    "else:\n",
    "    print(\"No data was processed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consolidating individual csv files into 2 files\n",
    "- energy_states_consolidated \n",
    "- max_states_consolidated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directory containing the CSV files\n",
    "directory = 'reports\\\\new_csv'\n",
    "\n",
    "# List all CSV files in the directory\n",
    "energy_csv_files = [file for file in os.listdir(directory) if file.endswith('.csv') and file.startswith('energy')]\n",
    "max_csv_files = [file for file in os.listdir(directory) if file.endswith('.csv') and file.startswith('max')]\n",
    "\n",
    "# Read each CSV file and concatenate them into a single DataFrame\n",
    "energy_dfs = []\n",
    "for file in energy_csv_files:\n",
    "    df = pd.read_csv(os.path.join(directory, file))\n",
    "    energy_dfs.append(df)\n",
    "\n",
    "max_dfs = []\n",
    "for file in max_csv_files:\n",
    "    df = pd.read_csv(os.path.join(directory, file))\n",
    "    max_dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "energy_result = pd.concat(energy_dfs, ignore_index=True)\n",
    "energy_result.reset_index(drop=True, inplace=True)\n",
    "max_result = pd.concat(max_dfs, ignore_index=True)\n",
    "max_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Write the consolidated DataFrame to a new CSV file\n",
    "energy_result.to_csv('energy_states_consolidated.csv', index=False)\n",
    "max_result.to_csv('max_states_consolidated.csv', index=False)\n",
    "\n",
    "print(\"Consolidated CSV file saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
